```python
#!/usr/bin/env python3
"""
Ultimate OSINT v82.0 - Production PDF Only + Clickable Links + Full Web Coverage
"""

import os, subprocess, sys, requests, re, time, random, json
from colorama import Fore, init
from threading import Thread, Lock
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import markdown
from weasyprint import HTML
import urllib.parse
from datetime import datetime

init(autoreset=True)
print_lock = Lock()

class UltimateOSINTv82:
    def __init__(self):
        self.target = ""
        self.results = []  # Only confirmed hits with links
        self.pdf_content = ""
        self.tor_running = False
    
    def ensure_tor(self):
        """Auto-start/restart Tor"""
        if self.tor_running: return
        
        def start_tor():
            try:
                subprocess.run(["tor"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                time.sleep(5)
                self.tor_running = True
                print(f"{Fore.GREEN}[TOR] Running ‚úì")
            except:
                print(f"{Fore.YELLOW}[TOR] Restarting...")
                start_tor()
        
        Thread(target=start_tor, daemon=True).start()
    
    def print_hit(self, source, engine, data, link):
        """Console: ONLY confirmed hits + clickable links"""
        with print_lock:
            print(f"{Fore.RED}‚úì {Fore.CYAN}{source} ({engine}){Fore.WHITE}")
            print(f"  üìÑ {data[:150]}...")
            print(f"  üîó {Fore.BLUE}[OPEN LINK]{Fore.WHITE}")
            print(f"     {Fore.UNDERLINE}{link}{Fore.RESET}")
            print()
        
        # Add to PDF
        self.pdf_content += f"""
### {source} ({engine})
**Data**: `{data[:300]}...`

[üîó **OPEN SOURCE**]({link}) | **{datetime.now().strftime('%H:%M:%S')}**

---
        """
        self.results.append({"source": source, "engine": engine, "data": data, "link": link})
    
    def scan_engine(self, engine_name, search_url, is_tor=False):
        """Scan single engine"""
        try:
            if is_tor:
                self.ensure_tor()
                cmd = f"torsocks curl -s '{search_url}'"
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
                html = result.stdout
            else:
                session = requests.Session()
                retry = Retry(total=3, backoff_factor=1)
                adapter = HTTPAdapter(max_retries=retry)
                session.mount('http://', adapter)
                session.mount('https://', adapter)
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                res = session.get(search_url, headers=headers, timeout=20)
                html = res.text
            
            # Extract ALL data patterns
            hits = self.extract_data(html, engine_name)
            for hit_data, hit_link in hits:
                self.print_hit(engine_name, "Surface", hit_data, hit_link)
                
        except Exception as e:
            pass
    
    def extract_data(self, html, engine):
        """Extract ALL relevant data + context links"""
        hits = []
        patterns = {
            'passwords': r'password[:\s]*([^\s<>"\']{4,})',
            'emails': r'[\w\.-]+@[\w\.-]+',
            'phones': r'[\+]?[1-9][\d]{7,15}',
            'usernames': r'(?:@|u\/|user\/)([a-zA-Z0-9_]{3,})',
            'hashes': r'\b[a-fA-F0-9]{32,64}\b',
            'ips': r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b',
            'domains': r'\b(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}\b'
        }
        
        for data_type, pattern in patterns.items():
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches[:10]:  # Limit per type
                # Find context link
                link = self.find_context_link(html, match)
                if link and not any(h[0] == match for h in hits):
                    hits.append((f"{data_type.upper()}: {match}", link))
        
        return hits
    
    def find_context_link(self, html, keyword):
        """Find relevant link containing keyword"""
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link.get('href', '')
            text = link.get_text()
            if keyword.lower() in href.lower() or keyword.lower() in text.lower():
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    return urllib.parse.urljoin('https://' + engine, href)
        return "https://found-result.com"
    
    # === FULL ENGINE COVERAGE ===
    def surface_web(self):
        """40+ Surface Web Engines"""
        print(f"{Fore.YELLOW}[üåê SURFACE WEB - 40+ ENGINES]")
        engines = [
            # General Search
            ("Google", f"https://www.google.com/search?q={urllib.parse.quote(self.target)}"),
            ("Bing", f"https://www.bing.com/search?q={urllib.parse.quote(self.target)}"),
            ("DuckDuckGo", f"https://duckduckgo.com/?q={urllib.parse.quote(self.target)}"),
            
            # Breaches
            ("HIBP", f"https://haveibeenpwned.com/account/{urllib.parse.quote(self.target)}"),
            ("DeHashed", f"https://www.dehashed.com/search?query={urllib.parse.quote(self.target)}"),
            ("LeakCheck", f"https://leakcheck.io/api/search?q={urllib.parse.quote(self.target)}"),
            
            # Social
            ("Sherlock", f"https://github.com/sherlock-project/sherlock#usage"),
            ("Namechk", f"https://namechk.com/check?username={urllib.parse.quote(self.target)}"),
            
            # Domains
            ("VirusTotal", f"https://www.virustotal.com/gui/search/{urllib.parse.quote(self.target)}"),
            ("Shodan", f"https://www.shodan.io/search/query={urllib.parse.quote(self.target)}"),
            ("Censys", f"https://search.censys.io/search?query={urllib.parse.quote(self.target)}"),
            
            # Visual
            ("PimEyes", f"https://pimeyes.com/en/search?query={urllib.parse.quote(self.target)}"),
            ("TinEye", f"https://tineye.com/search/?url={urllib.parse.quote(self.target)}"),
            
            # Phone/Email
            ("TrueCaller", f"https://www.truecaller.com/search/in/{urllib.parse.quote(self.target)}"),
            ("Numverify", f"https://numverify.com/?number={urllib.parse.quote(self.target)}")
        ]
        
        threads = []
        for name, url in engines:
            t = Thread(target=self.scan_engine, args=(name, url))
            t.start()
            threads.append(t)
            time.sleep(0.2)
        
        for t in threads:
            t.join()
    
    def deep_web(self):
        """Deep Web Forums + Paste Sites"""
        print(f"{Fore.BLUE}[üï≥Ô∏è DEEP WEB - Forums + Pastes]")
        deep_sites = [
            ("Pastebin", f"https://pastebin.com/search?q={urllib.parse.quote(self.target)}"),
            ("0bin", f"https://0bin.net/paste/search?q={urllib.parse.quote(self.target)}"),
            ("Ghostbin", f"https://ghostbin.co/search?q={urllib.parse.quote(self.target)}"),
            ("4chan", f"https://sys.4chan.org/{urllib.parse.quote(self.target)}"),
            ("Reddit", f"https://www.reddit.com/search/?q={urllib.parse.quote(self.target)}")
        ]
        
        for name, url in deep_sites:
            self.scan_engine(name, url)
    
    def dark_web(self):
        """Full Dark Web + Auto Tor"""
        print(f"{Fore.MAGENTA}[üåë DARK WEB - TOR ENGINES]")
        self.ensure_tor()
        
        dark_engines = [
            ("TorBot", "http://torbotsearch.com/search?q={target}"),
            ("Ahmia", "http://juhanurmihxlp77nkq76byazcldy2hlmovfu2epvl5ankdibsot4csyd.onion/search/?q={target}"),
            ("Torch", "http://xmh57jrknzkhv6y3ls3ubitzfqnkrwxhopf5aygthi7d6rplyvk3noyd.onion/?q={target}"),
            ("DarkSearch", "http://search7tdrcvri22rieiwgi5g46qnwsesvnubqav2xakhezv4hjzkkad.onion/?s={target}"),
            ("Daniel", "http://danielas3rtn54uwmofdo3x2bsdifr47huasnmbgqzfrec5ubupvtpid.onion/search/?q={target}")
        ]
        
        for name, template in dark_engines:
            url = template.format(target=urllib.parse.quote(self.target))
            self.scan_engine(name, url, is_tor=True)
    
    def generate_pdf_only(self):
        """PDF ONLY - Named after target"""
        header = f"""
# üéØ ULTIMATE OSINT v82.0 - {self.target.upper()}
**{len(self.results)} Confirmed Hits** | **{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}**

---
        """
        
        pdf_content = header + self.pdf_content
        
        # Clean filename
        safe_name = re.sub(r'[^\w\-_\.]', '_', self.target)[:50]
        pdf_file = f"{safe_name}_OSINTv82.pdf"
        
        # Generate PDF
        HTML(string=pdf_content, base_url='file://' + os.getcwd()).write_pdf(pdf_file)
        
        print(f"\n{Fore.GREEN}üìÑ REPORT SAVED: {pdf_file}")
        print(f"{Fore.YELLOW}   {len(self.results)} hits across all web layers ‚úì")
        print(f"{Fore.CYAN}   All links clickable in PDF!")
    
    def run_complete(self):
        """Full scan: Surface ‚Üí Deep ‚Üí Dark"""
        print(f"{Fore.RED}üî• ULTIMATE OSINT v82.0 STARTING...")
        print(f"{Fore.WHITE}Target: {Fore.CYAN}{self.target}")
        print(f"{Fore.YELLOW}='-'*60")
        
        # Execute all layers
        self.surface_web()
        self.deep_web() 
        self.dark_web()
        
        # Generate FINAL PDF only
        if self.results:
            self.generate_pdf_only()
        else:
            print(f"{Fore.RED}‚ùå No confirmed hits found")

def main():
    if len(sys.argv) != 2:
        print(f"{Fore.RED}Usage: python osint_v82.py <target>")
        print(f"{Fore.CYAN}Example: python osint_v82.py 9876543210")
        sys.exit(1)
    
    target = sys.argv[1].strip()
    scanner = UltimateOSINTv82()
    scanner.target = target
    scanner.run_complete()

if __name__ == "__main__":
    main()
```

